{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FARIS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FARIS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FARIS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow_hub as tf_hub\n",
    "from tensorflow.keras.models import load_model\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. MODEL LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('deployment\\\\best_model.h5', custom_objects={'KerasLayer': tf_hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. DATA CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"**New Film Showcases One Year of Unprecedented Creativity**\n",
    "\n",
    "In a year that has seen the world grapple with unprecedented challenges, one film has emerged as a beacon of hope and resilience. The new show, aptly titled \"One Year\", is set to premiere this weekend, promising to take viewers on a cinematic journey like no other.\n",
    "\n",
    "\"One Year\" is not just a film; it's a testament to the indomitable spirit of creativity that thrives even in the most trying times. The film chronicles the experiences of a diverse group of artists over the course of a year, capturing their struggles, triumphs, and the extraordinary art they produce against all odds.\n",
    "\n",
    "The new show is already generating buzz in the entertainment industry, with critics praising its raw, unfiltered portrayal of the artistic process. The film's unique narrative structure, which unfolds over the course of a year, offers a fresh perspective on the timeless themes of perseverance and the transformative power of art.\n",
    "\n",
    "As the world eagerly awaits the premiere of \"One Year\", there's no doubt that this film is set to be one of the most talked-about shows of the year. So, mark your calendars and prepare to be inspired by this captivating new show that celebrates the enduring spirit of creativity. Stay tuned for more updates!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    ''' \n",
    "    Preprocessing text by applying lowercasing, normalization, tokenization, stopword removal, and lemmatization\n",
    "    '''\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize the text\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove whitespaces\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Get the English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['also', 'said', 'would', 'could']) # add some frequent words that won't help the model\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Combine tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Text' column in the data\n",
    "text_processed = preprocessing(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# The model expects input data in batch, even if just predicting on one sample\n",
    "# So, I'll add an extra dimension with np.expand_dims\n",
    "preprocessed_article = np.expand_dims(text_processed, axis=0)\n",
    "\n",
    "# get the prediction\n",
    "predictions = model.predict(preprocessed_article)\n",
    "\n",
    "# get the class with the highest probability\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 is entertaintment, the model predicted correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
